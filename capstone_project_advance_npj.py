# -*- coding: utf-8 -*-
"""Capstone Project_Advance_npj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T6zvWFREko-yhsKShfTuPr7pDM9x1F49

# **Data Background**
2021 & 2022 submission data was cleaned and grouped by agents in the intermediate capstone project.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

data = pd.read_csv('/content/CLS22.csv', sep=',')

"""# **Data Exploration**"""

data.head()

data.info()

data.describe()

data.shape # total 414 agents

data.columns

"""# **Data Cleaning**

**Handling Missing Values**
"""

#check for missing values, no missing values
data.isnull().sum()

#add on column to categorise agents as High, Low. This will be the Target Column.

# create conditions
conditions = [
    (data['AFYP_Regular'] <= 100000),
    (data['AFYP_Regular'] > 100000)
    ]

# create a list of the values assign for each condition
values = ['Low', 'High']

# create a new column and assign values to it
data['Agent_Category'] = np.select(conditions, values)

data.head()

# Calculate agent experience

data['AGENT_JOIN_YEAR'] = 2023 - data['AGENT_JOIN_YEAR']

# Rename AGENT_JOIN_YEAR to Agent_Experience
data1 = data.rename(columns = {"AGENT_JOIN_YEAR":"Agent_Experience"} )
data1.head(20)

"""**Handling Duplicates**"""

# Check for duplicates. There is no duplicates
data.duplicated().sum()

"""**Handling Outliers**

There are outliers detected for 5 columns
"""

plt.figure(figsize=(14,6))
data1.boxplot()

plt.boxplot(data1['Cases'])

plt.boxplot(data1['Agent_Experience'])

"""**Check for Data Types**"""

data1.info()

# Convert the Agent_Category column to categorical using Label Encoding technique
# High=0, Low=1

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

# Assigning numerical values and storing in another column
data1['Agent_Category'] = labelencoder.fit_transform(data1['Agent_Category'])
data1.head(20)

# convert Agent_Category int into categorical data
data1['Agent_Category']=pd.Categorical(data1['Agent_Category'])
data1.dtypes

#drop column
data2=data1.drop(['AGENT_CODE','GAM_CODE','REGION_NAME'],axis=1)
data2.head()

"""**Check for Imbalance data**"""

data2['Agent_Category'].value_counts()
# this is an imbalanced data

"""Balance the data by using oversampling. Increase minority class samples using SMOTE (Synthetic Minority Oversampling Technique)."""

# Before apply SMOTE

X, y = data2[['AFYP_Regular','Cases','Average_Case_Size','Agent_Experience','AFYP_exclude_RTU']], data2['Agent_Category']

counter = Counter(y)
for k,v in counter.items():
 per = v / len(y) * 100
 print('Class=%d, n=%d (%.3f%%)' % (k, v, per))

X.shape

plt.bar(counter.keys(), counter.values())
plt.show()

X

from imblearn.over_sampling import SMOTE

#transform dataset
oversample = SMOTE()
X, Y = oversample.fit_resample(X,y)

#summarize the new class distribution
counter = Counter(Y)
print(counter)

X.shape

y.shape

Y.shape

plt.bar(counter.keys(), counter.values())
plt.show()

"""# **Scaling of Data**"""

#Normalizing the data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
normalised = sc.fit_transform(X)
print('Normalized data:')
data2_normalised = pd.DataFrame(normalised,columns=X.columns)
data2_normalised.head()

"""# **Data Splitting**"""

X.shape

y.shape

# train test split of model

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 0)

"""# **Create FNN Model**"""

import keras
from keras.models import Sequential
from keras.layers import Dense

X_train.shape

np.unique(Y,return_counts=True) # to check balance data

# create model, output type: binary outcome, Loss function: binary cross entrophy

model = Sequential()
model.add(Dense(256, input_dim=5, activation='relu'))  # input layer
model.add(Dense(256, activation='relu')) # hidden layer
model.add(Dense(128, activation='relu')) # hidden layer
model.add(Dense(1, activation='sigmoid')) # output layer

#To visualize neural network
model.summary()

# create a loss function
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

X.shape

Y.shape

680/20

history = model.fit(X_train, y_train, epochs=25, batch_size=90)

y_pred = model.predict(X_test)

#Converting predictions to label
pred = list()
for i in range(len(y_pred)):
    pred.append(np.argmax(y_pred[i]))

test  = y_test.to_list()
print(test)

from sklearn.metrics import accuracy_score
a = accuracy_score(pred,test)
print('Accuracy is:', a*100)

#Using test data as validation data.
history1 = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=25, batch_size=90)

plt.plot(history1.history['accuracy'])
plt.plot(history1.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history1.history['loss'])
plt.plot(history1.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""**This is an Unrepresentative Dataset. Most probably because of the number of samples in a dataset is too small (680,5)**


"""